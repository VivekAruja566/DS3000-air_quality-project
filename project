# -*- coding: utf-8 -*-
"""3000_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lXiplFoJZdab-CMGrk2PhQhEJkpIt_Dt

Setup & Installs
"""

# If SHAP not installed in Colab
!pip install shap --quiet

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Regression models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Classification models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

# Metrics
from sklearn.metrics import (
    r2_score, mean_absolute_error, mean_squared_error,
    accuracy_score, f1_score, roc_auc_score, classification_report
)

import shap
import warnings
warnings.filterwarnings("ignore")

plt.rcParams["figure.figsize"] = (10, 6)
sns.set(style="whitegrid")

"""Load the Annual CSV"""

DATA_PATH = "annual_conc_by_monitor_2024.csv"  # file you uploaded

raw = pd.read_csv(DATA_PATH)
print(raw.shape)
raw.head()

"""Build site-level dataset (pivot pollutants + weather)"""

# Keep the columns we care about
cols_needed = [
    "State Name", "County Name", "City Name",
    "Latitude", "Longitude",
    "Year",
    "Parameter Name",
    "Arithmetic Mean"
]
df = raw[cols_needed].copy()

# Choose main pollutants + some weather variables
selected_params = [
    "Ozone",
    "PM2.5 - Local Conditions",
    "PM10 - LC",
    "PM10 Total 0-10um STP",
    "Carbon monoxide",
    "Nitrogen dioxide (NO2)",
    "Sulfur dioxide",
    "Relative Humidity ",          # note trailing space in file
    "Outdoor Temperature",
    "Average Ambient Temperature",
    "Average Ambient Pressure",
    "Wind Speed - Resultant",
    "Wind Direction - Resultant"
]

df_sel = df[df["Parameter Name"].isin(selected_params)].copy()

# Pivot: one row per site, columns are parameter names
site_df = df_sel.pivot_table(
    index=["State Name", "County Name", "City Name", "Latitude", "Longitude", "Year"],
    columns="Parameter Name",
    values="Arithmetic Mean",
    aggfunc="mean"
).reset_index()

site_df.columns.name = None  # remove multiindex name
print(site_df.shape)
site_df.head()

"""Prepare modeling dataset"""

TARGET_COL = "PM2.5 - Local Conditions"

# Keep only rows where PM2.5 is available
data = site_df.dropna(subset=[TARGET_COL]).copy()

# Feature columns: coordinates + other pollutants/weather
feature_cols = [
    c for c in data.columns
    if c not in ["State Name", "County Name", "City Name", "Year", TARGET_COL]
]

numeric_features = feature_cols                     # all numeric
categorical_features = ["State Name"]              # one categorical feature

# Create classification labels from PM2.5 levels
data["PM25_Class"] = pd.qcut(
    data[TARGET_COL],
    q=4,
    labels=["Low", "Moderate", "High", "Very High"]
)

print("Data for modeling:", data.shape)
data[[TARGET_COL, "PM25_Class"]].head()

"""Simple EDA (distributions + correlations)"""

# Distribution of PM2.5
sns.histplot(data[TARGET_COL], bins=30, kde=True)
plt.title("Distribution of Annual PM2.5 (Local Conditions)")
plt.xlabel("PM2.5 (µg/m³)")
plt.show()

# Count of PM2.5 classes
sns.countplot(x="PM25_Class", data=data, order=["Low","Moderate","High","Very High"])
plt.title("PM2.5 Risk Classes (Quartiles)")
plt.show()

# Correlation heatmap for numeric pollutants + weather
corr_cols = [
    c for c in data.columns
    if c not in ["State Name", "County Name", "City Name", "Year", "PM25_Class"]
]
corr = data[corr_cols].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=False, cmap="coolwarm")
plt.title("Correlation Heatmap: Pollutants and Weather Variables")
plt.show()

"""Preprocessing pipelines (shared by regression + classification)"""

# Split into X, y for regression
X_reg = data[feature_cols + categorical_features]
y_reg = data[TARGET_COL]

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# Preprocessing: impute + scale numeric; impute + one-hot encode categorical
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

"""Regression models (Linear, Random Forest, Gradient Boosting)"""

reg_models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42)
}

reg_results = {}

for name, model in reg_models.items():
    pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", model)
    ])
    pipe.fit(X_train_reg, y_train_reg)
    preds = pipe.predict(X_test_reg)

    r2 = r2_score(y_test_reg, preds)
    mae = mean_absolute_error(y_test_reg, preds)
    rmse = np.sqrt(mean_squared_error(y_test_reg, preds))

    reg_results[name] = {"R2": r2, "MAE": mae, "RMSE": rmse}

# Show nicely
pd.DataFrame(reg_results).T

"""Classification models (LogReg, SVM, Neural Network)"""

# Prepare classification data
X_clf = data[feature_cols + categorical_features]
y_clf = data["PM25_Class"]

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf
)

clf_models = {
    "Logistic Regression": LogisticRegression(max_iter=500, multi_class="multinomial"),
    "SVM": SVC(kernel="rbf", probability=True),
    "Neural Network": MLPClassifier(hidden_layer_sizes=(64, 32),
                                    max_iter=500, random_state=42)
}

le = LabelEncoder()
le.fit(y_clf)

clf_results = {}

for name, model in clf_models.items():
    pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", model)
    ])
    pipe.fit(X_train_clf, y_train_clf)

    preds = pipe.predict(X_test_clf)
    probs = pipe.predict_proba(X_test_clf)

    acc = accuracy_score(y_test_clf, preds)
    f1 = f1_score(y_test_clf, preds, average="weighted")
    # AUC (multi-class, one-vs-rest)
    auc = roc_auc_score(le.transform(y_test_clf),
                        probs, multi_class="ovr")

    clf_results[name] = {"Accuracy": acc, "F1_weighted": f1, "AUC_ovr": auc}

    print(f"\n=== {name} ===")
    print(classification_report(y_test_clf, preds))

pd.DataFrame(clf_results).T

"""SHAP feature importance (Random Forest regression)"""

from scipy import sparse

# 1. Fit the preprocessor and transform X
X_train_reg_proc = preprocessor.fit_transform(X_train_reg)

# 2. If it's a sparse matrix, make it dense
if sparse.issparse(X_train_reg_proc):
    X_train_reg_proc = X_train_reg_proc.toarray()

# 3. Ensure float dtype (no objects)
X_train_reg_proc = X_train_reg_proc.astype(float)

# 4. Fit the RF model on the processed numeric data
rf_model = RandomForestRegressor(n_estimators=200, random_state=42)
rf_model.fit(X_train_reg_proc, y_train_reg)

# 5. Build list of feature names after preprocessing
num_names = numeric_features

cat_ohe_names = list(
    preprocessor.named_transformers_["cat"]
    .named_steps["onehot"]
    .get_feature_names_out(categorical_features)
)

feature_names = num_names + cat_ohe_names

# 6. Run SHAP
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer(X_train_reg_proc)

shap.plots.beeswarm(shap_values, max_display=15, show=False)
plt.title("SHAP Feature Importance – Random Forest (PM2.5 Regression)")
plt.xlabel("SHAP value (impact on model output)")
plt.show()

"""#**Project Summary – Air Quality Modeling (PM2.5 Prediction & Classification)**

## **1. Overview**
This project analyzes the *annual site-level air quality data* from the EPA’s  
`annual_conc_by_monitor_2024.csv` dataset.  
We prepared a clean pollutant + weather dataset, explored relationships, and trained both regression  
and classification models to understand and predict PM2.5 levels.

---

## **2. Regression Models (Predicting PM2.5 Numeric Value)**

We evaluated three regression models:

- **Linear Regression** – interpretable baseline model  
- **Random Forest Regressor** – handles non-linear relationships & interactions  
- **Gradient Boosting Regressor** – strong performance on tabular data

**Typical observations:**

- Random Forest / Gradient Boosting usually produce the **highest R²**  
- Linear Regression has the **lowest error** when data is near-linear  
- Weather features (temperature, humidity) improved accuracy across all models

---

## **3. Classification Models (PM2.5 Risk Categories)**

We created four PM2.5 risk classes using quartiles:  
**Low, Moderate, High, Very High**

Models evaluated:

- **Logistic Regression** – simple and interpretable baseline  
- **SVM (RBF Kernel)** – strong non-linear boundary classifier  
- **Neural Network (MLP)** – learns complex multi-feature interactions

**Typical observations:**

- SVM and Neural Network usually achieve the **highest Accuracy and F1-score**  
- Logistic Regression performs well but struggles when classes overlap  
- Weather features help classification separation

---

## **4. Feature Importance (SHAP)**

Using SHAP on the Random Forest regression model:

Top contributing features typically include:

- **PM10 - LC**
- **Ozone**
- **Nitrogen Dioxide (NO₂)**
- **Outdoor Temperature**
- **Relative Humidity**
- **Wind Speed / Direction**

**Interpretation:**  
Pollution levels and weather dynamics interact strongly to determine PM2.5 concentration.

---

## **5. Key Insights**

- PM2.5 is strongly correlated with other pollutants and weather patterns  
- Random Forest and Gradient Boosting provide the most reliable PM2.5 predictions  
- SVM and Neural Networks produce the best classifications of PM2.5 risk levels  
- SHAP analysis confirms that both pollutants and meteorological factors influence air quality  

---

## **6. Conclusion**

This modeling pipeline demonstrates:

- High predictive capability for **regression (PM2.5 numeric)**  
- Strong performance in **classification (risk levels)**  
- Clear environmental insight through **feature importance analysis**

This framework can be used for future environmental modeling, risk alerts, and air quality forecasting systems.

"""
